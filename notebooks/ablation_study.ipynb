{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from helper import init_ipynb\n",
    "envfound = init_ipynb()\n",
    "\n",
    "DIR = os.environ[\"DIR_PATH\"] if envfound else None\n",
    "DEVICE = os.environ[\"DEVICE\"] if envfound else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've set API key :  f0555591d0410671711554a53411c6d7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/antoinemagron/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from models import (OpenAIGPT, HF_LLM, GenerationArg, Model)\n",
    "from evaluation import MCQBenchmark\n",
    "import gc\n",
    "from vllm.model_executor.parallel_utils.parallel_state import destroy_model_parallel\n",
    "import torch.distributed\n",
    "from typing import List, Callable\n",
    "import pandas as pd\n",
    "from models.qa_prompts import QA_PROMPTS\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def cot_prompt_template(n_shots: int, q: str) -> str:\n",
    "    template = QA_PROMPTS[\"cot_answer_align\"]\n",
    "    shots = '\\n'.join(template['shots'][:n_shots])\n",
    "    return f\"\"\"-system:\\n{template['system']}\\n{shots}{template['q_form'].format(q=q)}\"\"\"\n",
    "\n",
    "def direct_prompt_template(n_shots: int, q: str) -> str:\n",
    "    template = QA_PROMPTS[\"direct_answer_align\"]\n",
    "    shots = '\\n'.join(template['shots'][:n_shots])\n",
    "    return f\"\"\"-system:\\n{template['system']}\\n{shots}{template['q_form'].format(q=q)}\"\"\"\n",
    "\n",
    "def cot1_prompt_template(n_shots: int, q: str) -> str:\n",
    "    template = QA_PROMPTS[\"1cot_answer_align\"]\n",
    "    shots = '\\n'.join(template['shots'][:n_shots])\n",
    "    return f\"\"\"-system:\\n{template['system']}\\n{shots}{template['q_form'].format(q=q)}\"\"\"\n",
    "\n",
    "FLAGS = [\"Therefore\", \"correct\", \"answer\"]\n",
    "def match_score(sentence):\n",
    "    return len([flag for flag in FLAGS if flag in sentence]) / len(FLAGS)\n",
    "\n",
    "def simple_extract(ans_sentence):\n",
    "    sel = [l for l in [\"A\", \"B\", \"C\", \"D\", \"E\"] if l in ans_sentence]\n",
    "    return sel[0] if sel != [] else \"-1\"\n",
    "\n",
    "def csa2(pred):\n",
    "    if(type(pred) != str):\n",
    "        pred = pred.outputs[0].text\n",
    "    sent_text = nltk.sent_tokenize(pred.replace(\"\\n\", \";\"))\n",
    "    sentence_score = sorted([[sent, match_score(sent)] for sent in sent_text], key=lambda _: _[1], reverse=True)\n",
    "    if(len(sentence_score) == 0):\n",
    "        return \"-1\"\n",
    "    sentence, score = sentence_score[0]\n",
    "    if(score == 0):\n",
    "        return \"-1\"\n",
    "    return simple_extract(sentence)\n",
    "\n",
    "\n",
    "def write_cache(cache_file:str,\n",
    "                res,\n",
    "                targets:List[str]):\n",
    "    total_cache_file = \"docs/benchmarks_results/full/\" + cache_file\n",
    "    with open(total_cache_file) as f:\n",
    "        singleqa = [f\"{res[i].outputs[0].text}\\n\\n{targets[i]}\\n\\n{'*'*100}\" for i in range(len(targets))]\n",
    "        f.write(\"\\n\\n\".join(singleqa))\n",
    "    print(f\"Cached full results on : {cache_file}\")\n",
    "\n",
    "\n",
    "def benchmark(llm: Model,\n",
    "              benchnames: List[str],\n",
    "              pt: Callable[[str], str],\n",
    "              search_ans: Callable[[str], str],\n",
    "              cache_file:str=None):\n",
    "    \"\"\"\n",
    "        args :\n",
    "            - model: model to test\n",
    "            - benchnames : names of the benchmarks to run for the given model\n",
    "            - pt : prompt template that inputs a question and return the prompt to the model\n",
    "            - search_ans : a function that finds the answer to the MCQ in the result string\n",
    "    \"\"\"\n",
    "    model_res = []\n",
    "    for benchname in benchnames:\n",
    "        print(\"Evaluation on\", benchname)\n",
    "        benchmark = MCQBenchmark(\n",
    "            BENCHMARKS_PATHS[benchname],\n",
    "            pt ## call with the specific prompt template\n",
    "        )\n",
    "        res = benchmark.assess(llm)\n",
    "        \n",
    "        if(cache_file is not None):\n",
    "            ## if a cache_file is inputed => cache out the file\n",
    "            write_cache(\n",
    "                cache_file,\n",
    "                res,\n",
    "                [q[\"answer\"] for q in benchmark.mcq]\n",
    "            )\n",
    "            \n",
    "\n",
    "        answ = []\n",
    "        for r in res:\n",
    "            answ.append(search_ans(r))\n",
    "        \n",
    "        model_res.append(\n",
    "            sum(q[\"answer\"] in ans for ans, q in zip(answ, benchmark.mcq)) / len(answ)\n",
    "        )\n",
    "    return model_res\n",
    "\n",
    "\n",
    "def testbench(\n",
    "    benchnames:List[str]=[\"AES7\", \"AES8\"],\n",
    "    modnames:List[str]=[\"epfl-llm/meditron-7b\", \"meta-llama/Llama-2-7b-hf\"],\n",
    "    runargs:List[GenerationArg]=[],\n",
    "    prompt_template:Callable[[str], str]=lambda x : x,\n",
    "    search_ans:Callable[[str], str]=lambda x : x,\n",
    "    use_vllm:bool=True,\n",
    "    cache_file:str=None\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "    all_res = []\n",
    "    for mod in modnames:\n",
    "        print(\"<\" + \"-\" * 100 + \">\")\n",
    "        try : \n",
    "        ## creating and loading the model\n",
    "            llm = HF_LLM(\n",
    "                        mod,\n",
    "                        device=DEVICE,\n",
    "                        use_vllm=use_vllm,\n",
    "                        arg=GenerationArg(use_vllm=use_vllm)\n",
    "                    )\n",
    "            llm.load()\n",
    "\n",
    "            for runarg in runargs:\n",
    "                ## update the model with current runargs\n",
    "                llm.set_arg(runarg)\n",
    "\n",
    "                ## run the benchmarks\n",
    "                rr = [mod] \\\n",
    "                    + [v for k, v in runarg.attr.items()] \\\n",
    "                    + benchmark(llm, benchnames, prompt_template, search_ans, cache_file) \n",
    "                all_res.append(rr)\n",
    "            destroy_model_parallel()\n",
    "            del llm\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            # torch.distributed.destroy_process_group()\n",
    "        except Exception as e:    \n",
    "            ## unload the model\n",
    "            print(\"Unloading the model\")\n",
    "            destroy_model_parallel()\n",
    "            del llm\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            # torch.distributed.destroy_process_group()\n",
    "            raise Exception(e)\n",
    "    cols = [\"mod\"] + [k for k, v in runargs[0].attr.items()] + benchnames\n",
    "    return pd.DataFrame(all_res, columns=cols)\n",
    "\n",
    "BENCHMARKS_PATHS = {\n",
    "    \"MCQ\" : \"docs/benchmarks/mcq40/processed.json\",\n",
    "    \"AES7\" : \"docs/benchmarks/self_assessment/aes7_processed.json\",\n",
    "    \"AES8\" :  \"docs/benchmarks/self_assessment/aes8_processed.json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation studies\n",
    "\n",
    "\n",
    "### Ablation on number of shots for COT1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modnames = [\"epfl-llm/meditron-7b\", \"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Llama-2-7b-hf\"]\n",
    "benchnames = [\"AES7\", \"AES8\"]\n",
    "DETERMINISTIC = GenerationArg(\n",
    "        temperature=0.00001,\n",
    "        use_vllm=True,\n",
    "        topk=1,\n",
    "        topp=1,\n",
    "        max_new_token=512,\n",
    "        stop_seq=\"###\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---------------------------------------------------------------------------------------------------->\n",
      "INFO 05-13 13:57:59 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='epfl-llm/meditron-7b', tokenizer='epfl-llm/meditron-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-13 13:57:59 selector.py:51] Cannot use FlashAttention because the package is not found. Please install it for better performance.\n",
      "INFO 05-13 13:57:59 selector.py:25] Using XFormers backend.\n",
      "INFO 05-13 13:58:00 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 05-13 13:58:01 model_runner.py:104] Loading model weights took 12.5527 GB\n",
      "INFO 05-13 13:58:02 gpu_executor.py:94] # GPU blocks: 3797, # CPU blocks: 512\n",
      "INFO 05-13 13:58:03 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-13 13:58:03 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-13 13:58:08 model_runner.py:867] Graph capturing finished in 5 secs.\n",
      "Evaluation on AES7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:47<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on AES8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:48<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---------------------------------------------------------------------------------------------------->\n",
      "INFO 05-13 13:59:44 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='meta-llama/Meta-Llama-3-8B', tokenizer='meta-llama/Meta-Llama-3-8B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-13 13:59:45 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 05-13 13:59:46 model_runner.py:104] Loading model weights took 14.9595 GB\n",
      "INFO 05-13 13:59:48 gpu_executor.py:94] # GPU blocks: 13450, # CPU blocks: 2048\n",
      "INFO 05-13 13:59:48 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-13 13:59:48 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-13 13:59:53 model_runner.py:867] Graph capturing finished in 5 secs.\n",
      "Evaluation on AES7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:19<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on AES8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:20<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---------------------------------------------------------------------------------------------------->\n",
      "INFO 05-13 14:00:34 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='meta-llama/Llama-2-7b-hf', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 05-13 14:00:34 selector.py:51] Cannot use FlashAttention because the package is not found. Please install it for better performance.\n",
      "INFO 05-13 14:00:34 selector.py:25] Using XFormers backend.\n",
      "INFO 05-13 14:00:34 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 05-13 14:00:36 model_runner.py:104] Loading model weights took 12.5523 GB\n",
      "INFO 05-13 14:00:37 gpu_executor.py:94] # GPU blocks: 3819, # CPU blocks: 512\n",
      "INFO 05-13 14:00:37 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-13 14:00:37 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-13 14:00:42 model_runner.py:867] Graph capturing finished in 5 secs.\n",
      "Evaluation on AES7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:47<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on AES8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:48<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---------------------------------------------------------------------------------------------------->\n",
      "INFO 05-13 14:02:19 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='epfl-llm/meditron-7b', tokenizer='epfl-llm/meditron-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-13 14:02:19 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 05-13 14:02:21 model_runner.py:104] Loading model weights took 12.5523 GB\n",
      "INFO 05-13 14:02:21 gpu_executor.py:94] # GPU blocks: 3826, # CPU blocks: 512\n",
      "INFO 05-13 14:02:21 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-13 14:02:21 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-13 14:02:26 model_runner.py:867] Graph capturing finished in 5 secs.\n",
      "Evaluation on AES7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on AES8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:37<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---------------------------------------------------------------------------------------------------->\n",
      "INFO 05-13 14:03:42 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='meta-llama/Meta-Llama-3-8B', tokenizer='meta-llama/Meta-Llama-3-8B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-13 14:03:42 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 05-13 14:03:44 model_runner.py:104] Loading model weights took 14.9575 GB\n",
      "INFO 05-13 14:03:45 gpu_executor.py:94] # GPU blocks: 13450, # CPU blocks: 2048\n",
      "INFO 05-13 14:03:45 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-13 14:03:45 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-13 14:03:51 model_runner.py:867] Graph capturing finished in 5 secs.\n",
      "Evaluation on AES7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:30<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on AES8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:31<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---------------------------------------------------------------------------------------------------->\n",
      "INFO 05-13 14:04:54 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='meta-llama/Llama-2-7b-hf', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 05-13 14:04:54 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 05-13 14:04:55 model_runner.py:104] Loading model weights took 12.5513 GB\n",
      "INFO 05-13 14:04:56 gpu_executor.py:94] # GPU blocks: 3819, # CPU blocks: 512\n",
      "INFO 05-13 14:04:56 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-13 14:04:56 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-13 14:05:01 model_runner.py:867] Graph capturing finished in 5 secs.\n",
      "Evaluation on AES7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on AES8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---------------------------------------------------------------------------------------------------->\n",
      "INFO 05-13 14:07:32 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='epfl-llm/meditron-7b', tokenizer='epfl-llm/meditron-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-13 14:07:32 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 05-13 14:07:33 model_runner.py:104] Loading model weights took 12.5523 GB\n",
      "INFO 05-13 14:07:34 gpu_executor.py:94] # GPU blocks: 3826, # CPU blocks: 512\n",
      "INFO 05-13 14:07:34 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-13 14:07:34 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-13 14:07:39 model_runner.py:867] Graph capturing finished in 5 secs.\n",
      "Evaluation on AES7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:45<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on AES8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:47<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---------------------------------------------------------------------------------------------------->\n",
      "INFO 05-13 14:09:13 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='meta-llama/Meta-Llama-3-8B', tokenizer='meta-llama/Meta-Llama-3-8B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-13 14:09:13 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 05-13 14:09:15 model_runner.py:104] Loading model weights took 14.9575 GB\n",
      "INFO 05-13 14:09:16 gpu_executor.py:94] # GPU blocks: 13450, # CPU blocks: 2048\n",
      "INFO 05-13 14:09:16 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-13 14:09:16 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-13 14:09:22 model_runner.py:867] Graph capturing finished in 5 secs.\n",
      "Evaluation on AES7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:42<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on AES8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:42<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---------------------------------------------------------------------------------------------------->\n",
      "INFO 05-13 14:10:48 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='meta-llama/Llama-2-7b-hf', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 05-13 14:10:48 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 05-13 14:10:50 model_runner.py:104] Loading model weights took 12.5513 GB\n",
      "INFO 05-13 14:10:51 gpu_executor.py:94] # GPU blocks: 3819, # CPU blocks: 512\n",
      "INFO 05-13 14:10:51 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-13 14:10:51 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-13 14:10:56 model_runner.py:867] Graph capturing finished in 5 secs.\n",
      "Evaluation on AES7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [01:28<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on AES8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [01:28<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---------------------------------------------------------------------------------------------------->\n",
      "INFO 05-13 14:13:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='epfl-llm/meditron-7b', tokenizer='epfl-llm/meditron-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-13 14:13:53 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 05-13 14:13:55 model_runner.py:104] Loading model weights took 12.5523 GB\n",
      "INFO 05-13 14:13:56 gpu_executor.py:94] # GPU blocks: 3826, # CPU blocks: 512\n",
      "INFO 05-13 14:13:56 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-13 14:13:56 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-13 14:14:01 model_runner.py:867] Graph capturing finished in 5 secs.\n",
      "Evaluation on AES7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:47<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on AES8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---------------------------------------------------------------------------------------------------->\n",
      "INFO 05-13 14:15:34 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='meta-llama/Meta-Llama-3-8B', tokenizer='meta-llama/Meta-Llama-3-8B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-13 14:15:34 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 05-13 14:15:36 model_runner.py:104] Loading model weights took 14.9575 GB\n",
      "INFO 05-13 14:15:37 gpu_executor.py:94] # GPU blocks: 13450, # CPU blocks: 2048\n",
      "INFO 05-13 14:15:37 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-13 14:15:37 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-13 14:15:43 model_runner.py:867] Graph capturing finished in 5 secs.\n",
      "Evaluation on AES7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:47<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on AES8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:48<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---------------------------------------------------------------------------------------------------->\n",
      "INFO 05-13 14:17:19 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='meta-llama/Llama-2-7b-hf', tokenizer='meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 05-13 14:17:20 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 05-13 14:17:21 model_runner.py:104] Loading model weights took 12.5513 GB\n",
      "INFO 05-13 14:17:22 gpu_executor.py:94] # GPU blocks: 3819, # CPU blocks: 512\n",
      "INFO 05-13 14:17:22 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-13 14:17:22 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-13 14:17:27 model_runner.py:867] Graph capturing finished in 5 secs.\n",
      "Evaluation on AES7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [01:51<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on AES8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [01:52<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for shots in range(4):\n",
    "    resdf = testbench(benchnames=benchnames,\n",
    "              runargs=[DETERMINISTIC],\n",
    "              prompt_template=lambda q : cot1_prompt_template(shots, q),\n",
    "              search_ans=csa2,\n",
    "              modnames=modnames,\n",
    "              use_vllm=True)\n",
    "    resdf[\"shots\"] = shots\n",
    "    results.append(resdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(results).to_csv(\"docs/benchmarks_results/ablation_study.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "BENCHMARKS_PATHS = {\n",
    "    \"MCQ\" : \"docs/benchmarks/mcq40/processed.json\",\n",
    "    \"AES7\" : \"docs/benchmarks/self_assessment/aes7_processed.json\",\n",
    "    \"AES8\" :  \"docs/benchmarks/self_assessment/aes8_processed.json\",\n",
    "}\n",
    "\n",
    "\n",
    "human_accuracies = {\n",
    "    \"AES7\" : sum([_[\"human_accuracy\"] for _ in MCQBenchmark(BENCHMARKS_PATHS[\"AES7\"], lambda x : x).mcq]) / 100,\n",
    "    \"AES8\" : sum([_[\"human_accuracy\"] for _ in MCQBenchmark(BENCHMARKS_PATHS[\"AES8\"], lambda x : x).mcq]) / 100,\n",
    "}\n",
    "\n",
    "human_accuracies[\"AES\"] = sum(s for b, s in human_accuracies.items()) / len(human_accuracies)\n",
    "\n",
    "GPT_RES_AES7 = 0.41\n",
    "GPT_RES_AES8 = 0.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mod                  12\n",
       "temperature          12\n",
       "top_k                12\n",
       "top_p                12\n",
       "max_tokens           12\n",
       "presence_penalty     12\n",
       "frequency_penalty    12\n",
       "use_beam_search      12\n",
       "logprobs             12\n",
       "best_of              12\n",
       "stop                 12\n",
       "use_vllm             12\n",
       "AES7                 12\n",
       "AES8                 12\n",
       "shots                12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"docs/benchmarks_results/ablation_study.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_plot(of=df, against=\"temp\", benchnames=[\"AES7\", \"AES8\"]):\n",
    "    colors = [\"#a1dab4\", \"#41b6c4\", \"#2c7fb8\", \"#253494\", \"#ffffcc\"]\n",
    "    temps = []\n",
    "    for bench in benchnames:\n",
    "        temp = df[[\"mod\", bench, \"temperature\"]]\n",
    "        temp[\"bench\"] = bench\n",
    "        temp.columns = [\"mod\", \"accuracy\", \"temperature\", \"bench\"]\n",
    "        temps.append(temp)\n",
    "    df = pd.concat(temps)\n",
    "\n",
    "    df[\"lines\"] = df[\"mod\"].apply(process_mod_name) + \" on \" + df[\"bench\"]\n",
    "    \n",
    "    f, a = plt.subplots(figsize=(10, 5))\n",
    "    for bench, color in zip(benchnames, colors):\n",
    "        a.axhline(human_accuracies[bench], linestyle=\"--\", label=f\"human score {bench} \", color=color)\n",
    "    a.axhline((GPT_RES_AES7 + GPT_RES_AES8) / 2 , linestyle=\"--\", label=f\"GPT3.5 {bench} \", color=\"r\")\n",
    "    # a.axhline(0.2 , linestyle=\"--\", label=f\"chance\", color=\"b\")\n",
    "    g = sns.lineplot(data=df, x=against, y=\"accuracy\", hue=\"lines\")\n",
    "    g.set_xlabel(\"Generation temperature\")\n",
    "    g.set_ylabel(\"Accuracy on benchmark\")\n",
    "\n",
    "def process_mod_name(modname):\n",
    "    return \"-\".join(modname.split(\"/\")[1].split(\"-\")[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_against_temp(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
