{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the perplexities over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from helper import init_ipynb\n",
    "envfound = init_ipynb()\n",
    "\n",
    "DIR = os.environ[\"DIR_PATH\"] if envfound else None\n",
    "DEVICE = os.environ[\"DEVICE\"] if envfound else None\n",
    "API_KEY = os.environ[\"API_KEY\"] if envfound else None\n",
    "PLATFORM = os.environ[\"OS_TYPE\"] if envfound else None\n",
    "\n",
    "if(PLATFORM == \"Darwin\"):\n",
    "    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import load_from_disk\n",
    "from evaluate import load\n",
    "from torch.nn.functional import one_hot\n",
    "from typing import Dict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# ppl = load(\"perplexity\", module_type=\"metric\")\n",
    "testds = load_from_disk(\"docs/pmc_patiens_fil_test.hf\")\n",
    "\n",
    "MODEL_MAX_LENGTH = 1024\n",
    "\n",
    "def compute_loss(model: LlamaForCausalLM,\n",
    "                 input_texts: Dict[str, str],\n",
    "                 tokenizer: AutoTokenizer) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "        Override of loss computation.\n",
    "\n",
    "        args : \n",
    "            - model (AutoModelForCausalLM) :\n",
    "            - inputs :\n",
    "\n",
    "        returns :\n",
    "            loss value, torch tensor with grad_fn\n",
    "    \"\"\"\n",
    "    VOCAB_SIZE = len(tokenizer)\n",
    "    inputs = tokenizer(input_texts, max_length=MODEL_MAX_LENGTH, truncation=True, padding=\"max_length\", return_tensors=\"pt\").to(DEVICE) \n",
    "    predictions = model(**inputs).logits ## model run and extract logits\n",
    "    loss = CrossEntropyLoss()(predictions.float(),\n",
    "                              one_hot(\n",
    "                                  inputs[\"input_ids\"],\n",
    "                                  num_classes=VOCAB_SIZE\n",
    "                            ).float()\n",
    "                    ).cpu() ## Loss computation, comparing the logits and the one hot distrib\n",
    "    del input_texts\n",
    "    del predictions\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppl(mod):\n",
    "    model = LlamaForCausalLM.from_pretrained(mod).to(DEVICE)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(mod)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    losses = []\n",
    "    for sample in tqdm(testds[\"text\"]):\n",
    "        losses.append(\n",
    "            compute_loss(model, [sample], tokenizer).detach()\n",
    "        )\n",
    "\n",
    "    del model\n",
    "    del tokenizer\n",
    "    return 2**(sum(losses) / len(losses)).cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbdaa43e3694f38a1f1fa59eccae5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51360bfdcf38407281da49db631178c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppl_meditron_7b = ppl(\"epfl-llm/meditron-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for meditron 7b : 1.3506844489229484\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss for meditron 7b :\", ppl_meditron_7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0a7e9bd1c741128ec441a5b652fac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dabf8f6cae446aa0310c1c793f5c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppl_ll3 = ppl(\"meta-llama/Meta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for LLaMA 3 : 1.0753576460522802\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss for LLaMA 3 :\", ppl_ll3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_epitron_M7B_PMCo_e1 = ppl(\"cryptoni/epitron_baseline_M7B_PMCo_e1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for EPITRON.M7B.PMCo.E1 : 1.0552800471506047\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss for EPITRON.M7B.PMCo.E1 :\", ppl_epitron_M7B_PMCo_e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a324b027ff04c55a889727861766f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d3355644724154aa7591e66155c30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppl_epitron_M7B_PMCo_e5 = ppl(\"cryptoni/epitron_baseline_PMCo_M7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for EPITRON.M7B.PMCo.E5 : 1.0552412232446187\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss for EPITRON.M7B.PMCo.E5 :\", ppl_epitron_M7B_PMCo_e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce582277eef944f49e9e6d33bdda6ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96361a7e484f46e5b8a992d22d03e7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppl_epitron_LL3_8B_PMCo_e1 = ppl(\"cryptoni/epitron_LL3_8B_PMCo_e1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for EPITRON.LL3.8B.PMCo.E1 : 1.0159650442256611\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss for EPITRON.LL3.8B.PMCo.E1 :\", ppl_epitron_LL3_8B_PMCo_e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3203619ef84d45a89580935a644cb376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a784cc24c4cd4894ba8299d104024f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppl_epitron_M7B_PMCo_e3 = ppl(\"cryptoni/epitron_baseline_PMCo_M7B_e3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for EPITRON.M7B.PMCo.E3 : 1.0552418390534275\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss for EPITRON.M7B.PMCo.E3 :\", ppl_epitron_M7B_PMCo_e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|model|ppl|dataset|\n",
    "|:------:|:---------:|:-------:|\n",
    "|Meditron-7B|1.35|pmc test|\n",
    "|LLaMA-3-8B|1.075|pmc test|\n",
    "|epitron.LL3.8B.PMCo.e1|1.01|pmc test|\n",
    "|epitron.M7B.PMCo.e1|1.06|pmc test|\n",
    "|epitron.M7B.PMCo.e3|1.055|pmc test|\n",
    "|epitron.M7B.PMCo.e5|1.055|pmc test|\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
